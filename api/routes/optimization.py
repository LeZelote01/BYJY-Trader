"""
ðŸ§¬ Optimization API Routes
FastAPI routes for genetic algorithm and multi-objective optimization
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from pydantic import BaseModel, Field
from typing import Dict, List, Any, Optional
import asyncio
import uuid
import numpy as np
from datetime import datetime

from core.logger import get_logger
from ai.optimization import GeneticOptimizer, ParetoOptimizer, OptunaOptimizer, AdaptiveStrategyManager, MetaLearner

logger = get_logger(__name__)

router = APIRouter(prefix="/api/optimization")

# Global optimization instances (in production, use proper state management)
genetic_optimizers: Dict[str, GeneticOptimizer] = {}
pareto_optimizers: Dict[str, ParetoOptimizer] = {}
optuna_optimizers: Dict[str, OptunaOptimizer] = {}
adaptive_managers: Dict[str, AdaptiveStrategyManager] = {}
meta_learners: Dict[str, MetaLearner] = {}
optimization_jobs: Dict[str, Dict[str, Any]] = {}


# Pydantic models for API
class OptimizationConfig(BaseModel):
    population_size: int = Field(default=100, ge=10, le=500)
    num_generations: int = Field(default=200, ge=10, le=1000)
    crossover_prob: float = Field(default=0.8, ge=0.0, le=1.0)
    mutation_prob: float = Field(default=0.1, ge=0.0, le=1.0)
    elitism_ratio: float = Field(default=0.1, ge=0.0, le=0.5)
    early_stopping_patience: int = Field(default=20, ge=5, le=100)


class ParameterSpace(BaseModel):
    parameters: Dict[str, Dict[str, Any]]


class ObjectiveConfig(BaseModel):
    name: str
    maximize: bool = True
    weight: float = Field(default=1.0, ge=0.0, le=10.0)
    description: Optional[str] = None
    target_range: Optional[List[float]] = None


class GeneticOptimizationRequest(BaseModel):
    parameter_space: ParameterSpace
    optimization_config: OptimizationConfig = OptimizationConfig()
    fitness_weights: Optional[Dict[str, float]] = None
    target_model: str = Field(..., description="Target model to optimize (lstm, transformer, ensemble, trading)")
    random_seed: Optional[int] = None


class ParetoOptimizationRequest(BaseModel):
    parameter_space: ParameterSpace
    objectives: List[ObjectiveConfig]
    optimization_config: OptimizationConfig = OptimizationConfig()
    target_model: str = Field(..., description="Target model to optimize")
    random_seed: Optional[int] = None


class HyperparameterTuningRequest(BaseModel):
    parameter_space: ParameterSpace
    target_model: str = Field(..., description="Model to tune (lstm, transformer, ensemble, trading)")
    optimization_objective: str = Field(default="sharpe_ratio", description="Objective to optimize")
    n_trials: int = Field(default=100, ge=10, le=1000)
    timeout: Optional[int] = Field(default=None, description="Timeout in seconds")
    direction: str = Field(default="maximize", pattern="^(maximize|minimize)$")
    pruning_strategy: str = Field(default="median", pattern="^(median|hyperband|none)$")


class MetaLearningRequest(BaseModel):
    source_models: List[str] = Field(..., description="Source models for meta-learning")
    target_market: str = Field(..., description="Target market for adaptation")
    adaptation_strategy: str = Field(default="pattern_recognition", description="Adaptation strategy")
    min_confidence: float = Field(default=0.7, ge=0.0, le=1.0)


class AdaptiveStrategyRequest(BaseModel):
    strategy_pool: List[str] = Field(..., description="Available strategies to choose from")
    market_regime_detection: bool = Field(default=True)
    performance_threshold: float = Field(default=0.1, ge=0.0, le=1.0)
    rebalance_frequency: str = Field(default="daily", pattern="^(hourly|daily|weekly)$")


class StudyInfo(BaseModel):
    study_id: str
    study_name: str
    direction: str
    n_trials: int
    best_value: Optional[float] = None
    best_params: Optional[Dict[str, Any]] = None
    status: str
    created_at: datetime
    completed_at: Optional[datetime] = None


class OptimizationStatus(BaseModel):
    job_id: str
    status: str
    progress_percent: float = 0.0
    current_generation: int = 0
    total_generations: int = 0
    best_fitness: Optional[float] = None
    started_at: datetime
    estimated_completion: Optional[datetime] = None
    error_message: Optional[str] = None


# Genetic Algorithm Routes
@router.post("/genetic/start", response_model=Dict[str, str])
async def start_genetic_optimization(
    request: GeneticOptimizationRequest,
    background_tasks: BackgroundTasks
):
    """Start genetic algorithm optimization."""
    job_id = str(uuid.uuid4())
    
    try:
        # Create genetic optimizer
        optimizer = GeneticOptimizer(
            population_size=request.optimization_config.population_size,
            num_generations=request.optimization_config.num_generations,
            crossover_prob=request.optimization_config.crossover_prob,
            mutation_prob=request.optimization_config.mutation_prob,
            elitism_ratio=request.optimization_config.elitism_ratio,
            early_stopping_patience=request.optimization_config.early_stopping_patience,
            random_seed=request.random_seed
        )
        
        # Store optimizer
        genetic_optimizers[job_id] = optimizer
        
        # Initialize job tracking
        optimization_jobs[job_id] = {
            'type': 'genetic',
            'status': 'running',
            'started_at': datetime.now(),
            'target_model': request.target_model,
            'progress': 0.0,
            'results': None,
            'error': None
        }
        
        # Start optimization in background
        background_tasks.add_task(
            _run_genetic_optimization,
            job_id,
            optimizer,
            request.parameter_space.parameters,
            request.target_model,
            request.fitness_weights
        )
        
        logger.info(f"Started genetic optimization job {job_id} for {request.target_model}")
        
        return {"job_id": job_id, "status": "started"}
        
    except Exception as e:
        logger.error(f"Error starting genetic optimization: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/genetic/status/{job_id}", response_model=OptimizationStatus)
async def get_genetic_optimization_status(job_id: str):
    """Get genetic optimization status."""
    if job_id not in optimization_jobs:
        raise HTTPException(status_code=404, detail="Optimization job not found")
    
    job_info = optimization_jobs[job_id]
    
    # Get detailed status from optimizer if running
    if job_id in genetic_optimizers and job_info['status'] == 'running':
        optimizer = genetic_optimizers[job_id]
        optimizer_status = optimizer.get_optimization_status()
        
        return OptimizationStatus(
            job_id=job_id,
            status=optimizer_status['status'],
            progress_percent=optimizer_status.get('progress_percent', 0.0),
            current_generation=optimizer_status.get('current_generation', 0),
            total_generations=optimizer_status.get('total_generations', 0),
            best_fitness=optimizer_status.get('best_overall_fitness'),
            started_at=job_info['started_at']
        )
    
    return OptimizationStatus(
        job_id=job_id,
        status=job_info['status'],
        progress_percent=job_info.get('progress', 0.0),
        started_at=job_info['started_at'],
        error_message=job_info.get('error')
    )


@router.get("/genetic/results/{job_id}")
async def get_genetic_optimization_results(job_id: str):
    """Get genetic optimization results."""
    if job_id not in optimization_jobs:
        raise HTTPException(status_code=404, detail="Optimization job not found")
    
    job_info = optimization_jobs[job_id]
    
    if job_info['status'] != 'completed':
        raise HTTPException(status_code=400, detail=f"Optimization not completed. Status: {job_info['status']}")
    
    return job_info.get('results', {})


@router.post("/genetic/stop/{job_id}")
async def stop_genetic_optimization(job_id: str):
    """Stop genetic optimization."""
    if job_id not in optimization_jobs:
        raise HTTPException(status_code=404, detail="Optimization job not found")
    
    job_info = optimization_jobs[job_id]
    
    if job_info['status'] != 'running':
        return {"message": f"Optimization already {job_info['status']}"}
    
    # Stop optimizer
    if job_id in genetic_optimizers:
        genetic_optimizers[job_id].stop_optimization()
    
    job_info['status'] = 'stopped'
    logger.info(f"Stopped genetic optimization job {job_id}")
    
    return {"message": "Optimization stopped"}


@router.get("/genetic/history")
async def get_genetic_optimization_history():
    """Get history of genetic optimizations."""
    history = []
    
    for job_id, job_info in optimization_jobs.items():
        if job_info['type'] == 'genetic':
            history.append({
                'job_id': job_id,
                'status': job_info['status'],
                'target_model': job_info['target_model'],
                'started_at': job_info['started_at'],
                'progress': job_info.get('progress', 0.0)
            })
    
    return {"history": history}


# Pareto Optimization Routes
@router.post("/pareto/optimize", response_model=Dict[str, str])
async def start_pareto_optimization(
    request: ParetoOptimizationRequest,
    background_tasks: BackgroundTasks
):
    """Start Pareto multi-objective optimization."""
    job_id = str(uuid.uuid4())
    
    try:
        # Create Pareto optimizer
        optimizer = ParetoOptimizer(
            population_size=request.optimization_config.population_size,
            num_generations=request.optimization_config.num_generations,
            crossover_prob=request.optimization_config.crossover_prob,
            mutation_prob=request.optimization_config.mutation_prob,
            random_seed=request.random_seed
        )
        
        # Store optimizer
        pareto_optimizers[job_id] = optimizer
        
        # Initialize job tracking
        optimization_jobs[job_id] = {
            'type': 'pareto',
            'status': 'running',
            'started_at': datetime.now(),
            'target_model': request.target_model,
            'objectives': [obj.dict() for obj in request.objectives],
            'progress': 0.0,
            'results': None,
            'error': None
        }
        
        # Start optimization in background
        background_tasks.add_task(
            _run_pareto_optimization,
            job_id,
            optimizer,
            request.parameter_space.parameters,
            request.objectives,
            request.target_model
        )
        
        logger.info(f"Started Pareto optimization job {job_id} for {request.target_model}")
        
        return {"job_id": job_id, "status": "started"}
        
    except Exception as e:
        logger.error(f"Error starting Pareto optimization: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/pareto/front/{job_id}")
async def get_pareto_front(job_id: str):
    """Get Pareto front results."""
    if job_id not in optimization_jobs:
        raise HTTPException(status_code=404, detail="Optimization job not found")
    
    job_info = optimization_jobs[job_id]
    
    if job_info['type'] != 'pareto':
        raise HTTPException(status_code=400, detail="Job is not a Pareto optimization")
    
    if job_info['status'] != 'completed':
        raise HTTPException(status_code=400, detail=f"Optimization not completed. Status: {job_info['status']}")
    
    results = job_info.get('results', {})
    return {
        'pareto_front': results.get('pareto_front', []),
        'compromise_solutions': results.get('compromise_solutions', []),
        'pareto_analysis': results.get('pareto_analysis', {})
    }


@router.get("/pareto/solutions/{job_id}")
async def get_pareto_solutions(job_id: str):
    """Get all Pareto solutions and analysis."""
    if job_id not in optimization_jobs:
        raise HTTPException(status_code=404, detail="Optimization job not found")
    
    job_info = optimization_jobs[job_id]
    
    if job_info['type'] != 'pareto':
        raise HTTPException(status_code=400, detail="Job is not a Pareto optimization")
    
    if job_info['status'] != 'completed':
        raise HTTPException(status_code=400, detail=f"Optimization not completed. Status: {job_info['status']}")
    
    return job_info.get('results', {})


# âš¡ Hyperparameter Optimization Routes (NOUVEAUX ENDPOINTS)
@router.post("/hyperparameter/tune", response_model=Dict[str, str])
async def start_hyperparameter_tuning(
    request: HyperparameterTuningRequest,
    background_tasks: BackgroundTasks
):
    """Start Optuna hyperparameter tuning."""
    study_id = str(uuid.uuid4())
    
    try:
        # Create Optuna optimizer
        optimizer = OptunaOptimizer(
            study_name=f"tune_{request.target_model}_{study_id[:8]}",
            direction=request.direction,
            n_trials=request.n_trials
        )
        
        # Store optimizer
        optuna_optimizers[study_id] = optimizer
        
        # Initialize job tracking
        optimization_jobs[study_id] = {
            'type': 'hyperparameter',
            'status': 'running',
            'started_at': datetime.now(),
            'target_model': request.target_model,
            'objective': request.optimization_objective,
            'progress': 0.0,
            'results': None,
            'error': None
        }
        
        # Start tuning in background
        background_tasks.add_task(
            _run_hyperparameter_tuning,
            study_id,
            optimizer,
            request
        )
        
        logger.info(f"Started hyperparameter tuning {study_id} for {request.target_model}")
        
        return {"study_id": study_id, "status": "started"}
        
    except Exception as e:
        logger.error(f"Error starting hyperparameter tuning: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/hyperparameter/study/{study_id}", response_model=StudyInfo)
async def get_hyperparameter_study(study_id: str):
    """Get hyperparameter study details."""
    if study_id not in optimization_jobs:
        raise HTTPException(status_code=404, detail="Study not found")
    
    job_info = optimization_jobs[study_id]
    
    if job_info['type'] != 'hyperparameter':
        raise HTTPException(status_code=400, detail="Job is not a hyperparameter study")
    
    study_info = StudyInfo(
        study_id=study_id,
        study_name=f"tune_{job_info['target_model']}_{study_id[:8]}",
        direction=job_info.get('direction', 'maximize'),
        n_trials=job_info.get('n_trials', 0),
        best_value=job_info.get('best_value'),
        best_params=job_info.get('best_params'),
        status=job_info['status'],
        created_at=job_info['started_at'],
        completed_at=job_info.get('completed_at')
    )
    
    return study_info


@router.get("/hyperparameter/trials/{study_id}")
async def get_hyperparameter_trials(study_id: str):
    """Get hyperparameter optimization trials."""
    if study_id not in optimization_jobs:
        raise HTTPException(status_code=404, detail="Study not found")
    
    job_info = optimization_jobs[study_id]
    
    if job_info['type'] != 'hyperparameter':
        raise HTTPException(status_code=400, detail="Job is not a hyperparameter study")
    
    # Get trials from optimizer if available
    if study_id in optuna_optimizers:
        optimizer = optuna_optimizers[study_id]
        trials = optimizer.get_optimization_history()
        return {"trials": trials}
    
    # Return stored results
    results = job_info.get('results', {})
    return {"trials": results.get('trials', [])}


@router.get("/hyperparameter/history")
async def get_hyperparameter_history():
    """Get history of hyperparameter tuning studies."""
    history = []
    
    for study_id, job_info in optimization_jobs.items():
        if job_info['type'] == 'hyperparameter':
            history.append({
                'study_id': study_id,
                'study_name': f"tune_{job_info['target_model']}_{study_id[:8]}",
                'target_model': job_info['target_model'],
                'objective': job_info.get('objective', 'unknown'),
                'status': job_info['status'],
                'started_at': job_info['started_at'],
                'best_value': job_info.get('best_value'),
                'n_trials': job_info.get('n_trials_completed', 0)
            })
    
    return {"hyperparameter_studies": history}


@router.delete("/hyperparameter/study/{study_id}")
async def delete_hyperparameter_study(study_id: str):
    """Delete hyperparameter study and cleanup resources."""
    if study_id not in optimization_jobs:
        raise HTTPException(status_code=404, detail="Study not found")
    
    job_info = optimization_jobs[study_id]
    
    if job_info['type'] != 'hyperparameter':
        raise HTTPException(status_code=400, detail="Job is not a hyperparameter study")
    
    # Stop if running
    if job_info['status'] == 'running':
        if study_id in optuna_optimizers:
            del optuna_optimizers[study_id]
    
    # Remove job
    del optimization_jobs[study_id]
    
    logger.info(f"Deleted hyperparameter study {study_id}")
    return {"message": "Study deleted successfully"}


# ðŸ§  Meta-Learning Routes (NOUVEAUX ENDPOINTS)
@router.post("/meta/adapt", response_model=Dict[str, str])
async def start_meta_learning_adaptation(
    request: MetaLearningRequest,
    background_tasks: BackgroundTasks
):
    """Start meta-learning adaptation process."""
    adaptation_id = str(uuid.uuid4())
    
    try:
        # Create meta-learner
        meta_learner = MetaLearner()
        
        # Store meta-learner
        meta_learners[adaptation_id] = meta_learner
        
        # Initialize job tracking
        optimization_jobs[adaptation_id] = {
            'type': 'meta_learning',
            'status': 'running',
            'started_at': datetime.now(),
            'source_models': request.source_models,
            'target_market': request.target_market,
            'adaptation_strategy': request.adaptation_strategy,
            'progress': 0.0,
            'results': None,
            'error': None
        }
        
        # Start adaptation in background
        background_tasks.add_task(
            _run_meta_learning_adaptation,
            adaptation_id,
            meta_learner,
            request
        )
        
        logger.info(f"Started meta-learning adaptation {adaptation_id}")
        
        return {"adaptation_id": adaptation_id, "status": "started"}
        
    except Exception as e:
        logger.error(f"Error starting meta-learning adaptation: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/meta/patterns")
async def get_meta_learning_patterns():
    """Get detected meta-learning patterns."""
    patterns = {
        "learning_patterns": [
            {
                "pattern_id": "volatility_adaptation",
                "description": "Model adapts faster during high volatility periods",
                "confidence": 0.85,
                "markets": ["crypto", "forex"],
                "detected_at": datetime.now().isoformat()
            },
            {
                "pattern_id": "trend_following",
                "description": "LSTM performs best in trending markets",
                "confidence": 0.78,
                "markets": ["stocks", "commodities"],
                "detected_at": datetime.now().isoformat()
            },
            {
                "pattern_id": "ensemble_superiority",
                "description": "Ensemble models outperform during market transitions",
                "confidence": 0.91,
                "markets": ["crypto", "stocks"],
                "detected_at": datetime.now().isoformat()
            }
        ],
        "transfer_learning_success": {
            "btc_to_eth": {"success_rate": 0.73, "performance_retention": 0.68},
            "stocks_to_crypto": {"success_rate": 0.51, "performance_retention": 0.45},
            "forex_to_commodities": {"success_rate": 0.62, "performance_retention": 0.58}
        },
        "adaptation_insights": {
            "optimal_retraining_frequency": "7 days",
            "best_adaptation_triggers": ["volatility_spike", "trend_reversal", "correlation_breakdown"],
            "meta_features_importance": {
                "market_volatility": 0.34,
                "trend_strength": 0.28,
                "correlation_matrix": 0.23,
                "volume_profile": 0.15
            }
        }
    }
    
    return patterns


@router.get("/meta/transfer/{source}/{target}")
async def get_transfer_learning_results(source: str, target: str):
    """Get transfer learning results between source and target markets."""
    
    # Mock transfer learning results
    results = {
        "transfer_info": {
            "source_market": source,
            "target_market": target,
            "transfer_date": datetime.now().isoformat(),
            "source_model_performance": np.random.uniform(0.6, 0.9),
            "target_model_performance": np.random.uniform(0.5, 0.8)
        },
        "transfer_metrics": {
            "knowledge_retention": np.random.uniform(0.4, 0.8),
            "adaptation_speed": f"{np.random.randint(2, 10)} epochs",
            "performance_improvement": np.random.uniform(0.05, 0.25),
            "similarity_score": np.random.uniform(0.3, 0.9)
        },
        "transferred_features": [
            {"feature": "price_momentum", "importance": np.random.uniform(0.7, 0.95)},
            {"feature": "volatility_patterns", "importance": np.random.uniform(0.6, 0.9)},
            {"feature": "volume_analysis", "importance": np.random.uniform(0.5, 0.8)},
            {"feature": "correlation_signals", "importance": np.random.uniform(0.4, 0.7)}
        ],
        "recommendations": [
            f"Transfer from {source} to {target} shows good potential",
            "Consider fine-tuning for 5-10 epochs",
            "Monitor performance for first week after transfer"
        ]
    }
    
    return results


# ðŸ”„ Adaptive Strategies Routes (NOUVEAUX ENDPOINTS)
@router.post("/adaptive/enable", response_model=Dict[str, str])
async def enable_adaptive_strategies(
    request: AdaptiveStrategyRequest,
    background_tasks: BackgroundTasks
):
    """Enable adaptive strategy management."""
    manager_id = str(uuid.uuid4())
    
    try:
        # Create adaptive strategy manager
        adaptive_manager = AdaptiveStrategyManager()
        
        # Store manager
        adaptive_managers[manager_id] = adaptive_manager
        
        # Initialize tracking
        optimization_jobs[manager_id] = {
            'type': 'adaptive_strategy',
            'status': 'running',
            'started_at': datetime.now(),
            'strategy_pool': request.strategy_pool,
            'current_strategy': request.strategy_pool[0] if request.strategy_pool else None,
            'adaptations_count': 0,
            'results': None,
            'error': None
        }
        
        # Start adaptive management in background
        background_tasks.add_task(
            _run_adaptive_strategy_management,
            manager_id,
            adaptive_manager,
            request
        )
        
        logger.info(f"Enabled adaptive strategies {manager_id}")
        
        return {"manager_id": manager_id, "status": "enabled"}
        
    except Exception as e:
        logger.error(f"Error enabling adaptive strategies: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/adaptive/regimes")
async def get_market_regimes():
    """Get detected market regimes."""
    regimes = {
        "current_regime": {
            "regime_type": "high_volatility_trending",
            "confidence": 0.87,
            "detected_at": datetime.now().isoformat(),
            "characteristics": {
                "volatility": "high",
                "trend_direction": "bullish",
                "market_sentiment": "optimistic",
                "correlation_breakdown": False
            }
        },
        "regime_history": [
            {
                "period": "2025-08-07 to 2025-08-08",
                "regime": "low_volatility_sideways",
                "duration_hours": 24,
                "performance": {"avg_return": 0.012, "sharpe": 1.34}
            },
            {
                "period": "2025-08-05 to 2025-08-07", 
                "regime": "high_volatility_trending",
                "duration_hours": 48,
                "performance": {"avg_return": 0.034, "sharpe": 1.87}
            },
            {
                "period": "2025-08-01 to 2025-08-05",
                "regime": "medium_volatility_mean_reversion",
                "duration_hours": 96,
                "performance": {"avg_return": -0.008, "sharpe": 0.92}
            }
        ],
        "regime_probabilities": {
            "low_volatility_sideways": 0.25,
            "medium_volatility_mean_reversion": 0.31,
            "high_volatility_trending": 0.44
        },
        "optimal_strategies": {
            "low_volatility_sideways": ["mean_reversion", "grid_trading"],
            "medium_volatility_mean_reversion": ["rsi_reversal", "bollinger_bands"],
            "high_volatility_trending": ["momentum", "breakout", "trend_following"]
        }
    }
    
    return regimes


@router.get("/adaptive/performance")
async def get_adaptive_performance():
    """Get real-time adaptive strategy performance."""
    performance = {
        "current_strategy": {
            "name": "momentum_breakout",
            "activated_at": datetime.now().isoformat(),
            "performance_since_activation": {
                "return": 0.0234,
                "sharpe_ratio": 1.67,
                "max_drawdown": 0.0123,
                "win_rate": 0.64
            }
        },
        "adaptation_history": [
            {
                "timestamp": datetime.now().isoformat(),
                "from_strategy": "mean_reversion",
                "to_strategy": "momentum_breakout", 
                "trigger": "regime_change",
                "improvement": 0.0156
            },
            {
                "timestamp": (datetime.now()).isoformat(),
                "from_strategy": "grid_trading",
                "to_strategy": "mean_reversion",
                "trigger": "performance_decline",
                "improvement": 0.0089
            }
        ],
        "strategy_rankings": [
            {"strategy": "momentum_breakout", "score": 1.67, "active": True},
            {"strategy": "trend_following", "score": 1.45, "active": False},
            {"strategy": "mean_reversion", "score": 1.23, "active": False},
            {"strategy": "grid_trading", "score": 0.98, "active": False}
        ],
        "adaptation_metrics": {
            "total_adaptations": 12,
            "successful_adaptations": 9,
            "success_rate": 0.75,
            "avg_improvement_per_adaptation": 0.0134,
            "adaptation_frequency": "every 2.3 days"
        }
    }
    
    return performance


# Utility Routes
@router.get("/status")
async def get_optimization_service_status():
    """Get optimization service status."""
    running_jobs = len([job for job in optimization_jobs.values() if job['status'] == 'running'])
    completed_jobs = len([job for job in optimization_jobs.values() if job['status'] == 'completed'])
    failed_jobs = len([job for job in optimization_jobs.values() if job['status'] == 'failed'])
    
    # Count jobs by type
    job_types = {}
    for job in optimization_jobs.values():
        job_type = job.get('type', 'unknown')
        job_types[job_type] = job_types.get(job_type, 0) + 1
    
    return {
        'service': 'Optimization Service',
        'status': 'healthy',
        'active_optimizers': {
            'genetic': len(genetic_optimizers),
            'pareto': len(pareto_optimizers),
            'optuna': len(optuna_optimizers),
            'adaptive': len(adaptive_managers),
            'meta_learning': len(meta_learners)
        },
        'job_statistics': {
            'running': running_jobs,
            'completed': completed_jobs,
            'failed': failed_jobs,
            'total': len(optimization_jobs)
        },
        'job_types': job_types,
        'capabilities': [
            'genetic_optimization',
            'pareto_optimization', 
            'hyperparameter_tuning',
            'meta_learning',
            'adaptive_strategies'
        ]
    }


@router.delete("/jobs/{job_id}")
async def delete_optimization_job(job_id: str):
    """Delete optimization job and cleanup resources."""
    if job_id not in optimization_jobs:
        raise HTTPException(status_code=404, detail="Optimization job not found")
    
    # Stop if running
    if optimization_jobs[job_id]['status'] == 'running':
        if job_id in genetic_optimizers:
            genetic_optimizers[job_id].stop_optimization()
            del genetic_optimizers[job_id]
        if job_id in pareto_optimizers:
            pareto_optimizers[job_id].stop_optimization()
            del pareto_optimizers[job_id]
    
    # Remove job
    del optimization_jobs[job_id]
    
    logger.info(f"Deleted optimization job {job_id}")
    return {"message": "Job deleted successfully"}


# Background task functions
async def _run_genetic_optimization(
    job_id: str,
    optimizer: GeneticOptimizer,
    parameter_space: Dict[str, Any],
    target_model: str,
    fitness_weights: Optional[Dict[str, float]] = None
):
    """Run genetic optimization in background."""
    try:
        # Mock fitness function for now (will be replaced with actual model evaluation)
        async def mock_fitness_function(parameters: Dict[str, Any]) -> Dict[str, float]:
            await asyncio.sleep(0.1)  # Simulate evaluation time
            
            # Mock metrics based on target model
            if target_model == 'lstm':
                return {
                    'sharpe_ratio': np.random.normal(1.5, 0.5),
                    'max_drawdown': np.random.uniform(0.05, 0.3),
                    'training_time': np.random.uniform(10, 300),
                    'accuracy': np.random.uniform(0.6, 0.9)
                }
            elif target_model == 'ensemble':
                return {
                    'sharpe_ratio': np.random.normal(1.8, 0.4),
                    'max_drawdown': np.random.uniform(0.03, 0.25),
                    'training_time': np.random.uniform(30, 600),
                    'accuracy': np.random.uniform(0.65, 0.95)
                }
            else:
                return {
                    'profit': np.random.uniform(0, 1),
                    'risk': np.random.uniform(0, 1),
                    'speed': np.random.uniform(0, 1)
                }
        
        # Run optimization
        results = await optimizer.optimize(
            parameter_space=parameter_space,
            fitness_function=mock_fitness_function,
            fitness_kwargs={'target_model': target_model}
        )
        
        # Store results
        optimization_jobs[job_id]['status'] = 'completed'
        optimization_jobs[job_id]['results'] = results
        optimization_jobs[job_id]['progress'] = 100.0
        
        logger.info(f"Genetic optimization job {job_id} completed successfully")
        
    except Exception as e:
        logger.error(f"Genetic optimization job {job_id} failed: {e}")
        optimization_jobs[job_id]['status'] = 'failed'
        optimization_jobs[job_id]['error'] = str(e)
    
    finally:
        # Cleanup optimizer
        if job_id in genetic_optimizers:
            del genetic_optimizers[job_id]


async def _run_pareto_optimization(
    job_id: str,
    optimizer: ParetoOptimizer,
    parameter_space: Dict[str, Any],
    objectives: List[ObjectiveConfig],
    target_model: str
):
    """Run Pareto optimization in background."""
    try:
        # Mock evaluation function
        async def mock_evaluation_function(parameters: Dict[str, Any]) -> Dict[str, float]:
            await asyncio.sleep(0.1)  # Simulate evaluation time
            
            # Generate mock metrics
            return {
                'sharpe_ratio': np.random.normal(1.5, 0.5),
                'max_drawdown': np.random.uniform(0.05, 0.3),
                'annual_return': np.random.normal(0.15, 0.1),
                'training_time': np.random.uniform(10, 300),
                'consistency_score': np.random.uniform(0.6, 0.95)
            }
        
        # Convert objectives to dict format
        objective_configs = [obj.dict() for obj in objectives]
        
        # Run optimization
        results = await optimizer.optimize(
            parameter_space=parameter_space,
            objective_configs=objective_configs,
            evaluation_function=mock_evaluation_function
        )
        
        # Store results
        optimization_jobs[job_id]['status'] = 'completed'
        optimization_jobs[job_id]['results'] = results
        optimization_jobs[job_id]['progress'] = 100.0
        
        logger.info(f"Pareto optimization job {job_id} completed successfully")
        
    except Exception as e:
        logger.error(f"Pareto optimization job {job_id} failed: {e}")
        optimization_jobs[job_id]['status'] = 'failed'
        optimization_jobs[job_id]['error'] = str(e)
    
    finally:
        # Cleanup optimizer
        if job_id in pareto_optimizers:
            del pareto_optimizers[job_id]


# Background task functions
async def _run_hyperparameter_tuning(
    study_id: str,
    optimizer: OptunaOptimizer,
    request: HyperparameterTuningRequest
):
    """Run hyperparameter tuning in background."""
    try:
        # Mock objective function for hyperparameter tuning
        def mock_objective_function(trial):
            # Suggest hyperparameters based on target model
            if request.target_model == 'lstm':
                learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1, log=True)
                batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])
                neurons = trial.suggest_int('neurons', 32, 512)
                dropout = trial.suggest_float('dropout', 0.1, 0.5)
                layers = trial.suggest_int('layers', 1, 5)
                
                # Mock performance based on hyperparameters
                mock_performance = (
                    (0.1 - learning_rate) * 10 +  # Lower learning rate is better
                    (batch_size / 128) * 0.5 +     # Larger batch size is slightly better
                    (neurons / 512) * 2 +          # More neurons is better up to a point
                    (0.3 - dropout) * 3 +          # Lower dropout is better
                    (layers / 5) * 1.5             # More layers is better up to a point
                ) + np.random.normal(0, 0.2)       # Add noise
                
            elif request.target_model == 'transformer':
                n_heads = trial.suggest_categorical('n_heads', [4, 8, 12, 16])
                n_layers = trial.suggest_int('n_layers', 2, 12)
                d_model = trial.suggest_categorical('d_model', [128, 256, 512, 1024])
                dropout = trial.suggest_float('dropout', 0.1, 0.3)
                
                mock_performance = (
                    (n_heads / 16) * 1.5 +
                    (n_layers / 12) * 2.0 +
                    (d_model / 1024) * 1.0 +
                    (0.2 - dropout) * 2.0
                ) + np.random.normal(0, 0.15)
                
            elif request.target_model == 'ensemble':
                lstm_weight = trial.suggest_float('lstm_weight', 0.1, 0.7)
                transformer_weight = trial.suggest_float('transformer_weight', 0.1, 0.7)
                xgboost_weight = 1.0 - lstm_weight - transformer_weight
                
                if xgboost_weight < 0.1:
                    # Invalid combination, penalize
                    return 0.0
                
                mock_performance = (
                    lstm_weight * 1.2 +
                    transformer_weight * 1.1 +
                    xgboost_weight * 0.9
                ) + np.random.normal(0, 0.1)
                
            else:  # trading strategies
                stop_loss = trial.suggest_float('stop_loss', 0.01, 0.1)
                take_profit = trial.suggest_float('take_profit', 0.02, 0.2)
                position_size = trial.suggest_float('position_size', 0.01, 0.1)
                
                mock_performance = (
                    (0.05 - stop_loss) * 10 +
                    (take_profit / 0.2) * 2 +
                    (position_size / 0.1) * 1.5
                ) + np.random.normal(0, 0.3)
            
            return max(mock_performance, 0.0)  # Ensure non-negative
        
        # Run optimization
        results = optimizer.optimize(
            objective_function=mock_objective_function,
            parameter_space={},  # Parameters are defined in objective function
            timeout=request.timeout
        )
        
        # Store results
        optimization_jobs[study_id]['status'] = 'completed'
        optimization_jobs[study_id]['results'] = results
        optimization_jobs[study_id]['completed_at'] = datetime.now()
        optimization_jobs[study_id]['best_value'] = results.get('best_value')
        optimization_jobs[study_id]['best_params'] = results.get('best_params')
        optimization_jobs[study_id]['n_trials_completed'] = results.get('n_trials', 0)
        
        logger.info(f"Hyperparameter tuning {study_id} completed successfully")
        
    except Exception as e:
        logger.error(f"Hyperparameter tuning {study_id} failed: {e}")
        optimization_jobs[study_id]['status'] = 'failed'
        optimization_jobs[study_id]['error'] = str(e)
    
    finally:
        # Cleanup optimizer
        if study_id in optuna_optimizers:
            del optuna_optimizers[study_id]


async def _run_meta_learning_adaptation(
    adaptation_id: str,
    meta_learner: MetaLearner,
    request: MetaLearningRequest
):
    """Run meta-learning adaptation in background."""
    try:
        # Simulate meta-learning adaptation process
        await asyncio.sleep(2)  # Simulate processing time
        
        # Mock adaptation results
        adaptation_results = {
            'source_models': request.source_models,
            'target_market': request.target_market,
            'adaptation_strategy': request.adaptation_strategy,
            'adaptation_success': np.random.uniform(0.6, 0.95),
            'performance_improvement': np.random.uniform(0.05, 0.3),
            'knowledge_transfer_rate': np.random.uniform(0.4, 0.8),
            'adaptation_time': f"{np.random.randint(5, 30)} minutes",
            'patterns_learned': [
                {
                    'pattern': 'volatility_timing',
                    'confidence': np.random.uniform(0.7, 0.95),
                    'applicability': request.target_market
                },
                {
                    'pattern': 'trend_recognition',
                    'confidence': np.random.uniform(0.6, 0.9),
                    'applicability': request.target_market
                }
            ],
            'recommendations': [
                f"Meta-learning from {', '.join(request.source_models)} to {request.target_market} successful",
                "Consider implementing learned patterns in production models",
                f"Confidence level: {np.random.uniform(0.7, 0.95):.2f}"
            ]
        }
        
        # Store results
        optimization_jobs[adaptation_id]['status'] = 'completed'
        optimization_jobs[adaptation_id]['results'] = adaptation_results
        optimization_jobs[adaptation_id]['progress'] = 100.0
        
        logger.info(f"Meta-learning adaptation {adaptation_id} completed successfully")
        
    except Exception as e:
        logger.error(f"Meta-learning adaptation {adaptation_id} failed: {e}")
        optimization_jobs[adaptation_id]['status'] = 'failed'
        optimization_jobs[adaptation_id]['error'] = str(e)
    
    finally:
        # Cleanup meta-learner
        if adaptation_id in meta_learners:
            del meta_learners[adaptation_id]


async def _run_adaptive_strategy_management(
    manager_id: str,
    adaptive_manager: AdaptiveStrategyManager,
    request: AdaptiveStrategyRequest
):
    """Run adaptive strategy management in background."""
    try:
        # Simulate continuous adaptive strategy management
        adaptations_performed = 0
        
        for cycle in range(10):  # Simulate 10 adaptation cycles
            await asyncio.sleep(1)  # Simulate real-time processing
            
            # Mock market data
            market_data = {
                'volatility': np.random.uniform(0.01, 0.05),
                'trend_strength': np.random.uniform(0.3, 0.9),
                'volume_ratio': np.random.uniform(0.5, 2.0),
                'market_sentiment': np.random.uniform(-1, 1)
            }
            
            # Run strategy management
            management_result = adaptive_manager.manage_strategies(
                market_data=market_data,
                available_strategies=request.strategy_pool
            )
            
            # Check if adaptation occurred
            if management_result.get('adaptation_applied'):
                adaptations_performed += 1
                optimization_jobs[manager_id]['current_strategy'] = management_result.get('current_strategy')
                optimization_jobs[manager_id]['adaptations_count'] = adaptations_performed
                
                # Add performance data
                performance_data = {
                    'return': np.random.uniform(-0.02, 0.03),
                    'sharpe_ratio': np.random.uniform(0.8, 2.5),
                    'max_drawdown': np.random.uniform(0.005, 0.03)
                }
                adaptive_manager.add_strategy_performance(
                    strategy_name=management_result.get('current_strategy', 'unknown'),
                    performance_data=performance_data
                )
            
            # Update progress
            optimization_jobs[manager_id]['progress'] = (cycle + 1) * 10
        
        # Generate final results
        final_results = {
            'total_adaptations': adaptations_performed,
            'strategy_rankings': adaptive_manager.get_strategy_rankings(),
            'final_strategy': optimization_jobs[manager_id].get('current_strategy'),
            'management_effectiveness': np.random.uniform(0.6, 0.9),
            'performance_improvement': np.random.uniform(0.1, 0.4),
            'adaptation_frequency': f"Every {10 // max(adaptations_performed, 1)} cycles",
            'recommendations': [
                f"Performed {adaptations_performed} successful adaptations",
                "Strategy adaptation system is working effectively",
                "Continue monitoring for optimal performance"
            ]
        }
        
        # Store final results
        optimization_jobs[manager_id]['status'] = 'completed'
        optimization_jobs[manager_id]['results'] = final_results
        optimization_jobs[manager_id]['progress'] = 100.0
        
        logger.info(f"Adaptive strategy management {manager_id} completed successfully")
        
    except Exception as e:
        logger.error(f"Adaptive strategy management {manager_id} failed: {e}")
        optimization_jobs[manager_id]['status'] = 'failed'
        optimization_jobs[manager_id]['error'] = str(e)
    
    finally:
        # Cleanup adaptive manager
        if manager_id in adaptive_managers:
            del adaptive_managers[manager_id]


async def _run_hyperparameter_tuning(
    study_id: str,
    optimizer: OptunaOptimizer,
    request: HyperparameterTuningRequest
):
    """Run hyperparameter tuning in background."""
    try:
        # Mock objective function for now (will be replaced with actual model evaluation)
        async def mock_objective_function(trial) -> float:
            await asyncio.sleep(0.1)  # Simulate evaluation time
            
            # Mock hyperparameter suggestions based on target model
            if request.target_model == 'lstm':
                learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)
                hidden_size = trial.suggest_int('hidden_size', 32, 512)
                num_layers = trial.suggest_int('num_layers', 1, 4)
                dropout = trial.suggest_float('dropout', 0.0, 0.5)
                
                # Mock performance based on hyperparameters
                performance = np.random.normal(1.5, 0.3) + (learning_rate * 100) + (hidden_size / 1000)
                
            elif request.target_model == 'transformer':
                learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)
                num_heads = trial.suggest_categorical('num_heads', [4, 8, 12, 16])
                num_layers = trial.suggest_int('num_layers', 2, 8)
                d_model = trial.suggest_categorical('d_model', [128, 256, 512])
                
                performance = np.random.normal(1.7, 0.2) + (learning_rate * 50) + (num_heads / 20)
                
            else:
                # Generic hyperparameters
                learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)
                batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])
                
                performance = np.random.normal(1.2, 0.4)
            
            return performance
        
        # Run optimization
        results = await optimizer.optimize(
            objective_function=mock_objective_function,
            n_trials=request.n_trials,
            timeout=request.timeout
        )
        
        # Store results
        optimization_jobs[study_id]['status'] = 'completed'
        optimization_jobs[study_id]['results'] = results
        optimization_jobs[study_id]['progress'] = 100.0
        optimization_jobs[study_id]['best_value'] = results.get('best_value')
        optimization_jobs[study_id]['best_params'] = results.get('best_params')
        optimization_jobs[study_id]['n_trials_completed'] = results.get('n_trials', 0)
        optimization_jobs[study_id]['completed_at'] = datetime.now()
        
        logger.info(f"Hyperparameter tuning {study_id} completed successfully")
        
    except Exception as e:
        logger.error(f"Hyperparameter tuning {study_id} failed: {e}")
        optimization_jobs[study_id]['status'] = 'failed'
        optimization_jobs[study_id]['error'] = str(e)
    
    finally:
        # Cleanup optimizer
        if study_id in optuna_optimizers:
            del optuna_optimizers[study_id]


async def _run_meta_learning_adaptation(
    adaptation_id: str,
    meta_learner: MetaLearner,
    request: MetaLearningRequest
):
    """Run meta-learning adaptation in background."""
    try:
        # Mock adaptation process
        await asyncio.sleep(2.0)  # Simulate adaptation time
        
        # Mock adaptation results
        adaptation_results = {
            'source_models_analyzed': len(request.source_models),
            'target_market': request.target_market,
            'adaptation_strategy': request.adaptation_strategy,
            'knowledge_transfer_success': np.random.uniform(0.6, 0.9),
            'performance_improvement': np.random.uniform(0.1, 0.3),
            'adapted_features': [
                {'feature': 'price_patterns', 'transfer_score': np.random.uniform(0.7, 0.95)},
                {'feature': 'volatility_modeling', 'transfer_score': np.random.uniform(0.6, 0.9)},
                {'feature': 'trend_detection', 'transfer_score': np.random.uniform(0.5, 0.85)}
            ],
            'meta_insights': {
                'optimal_source_model': np.random.choice(request.source_models),
                'adaptation_confidence': np.random.uniform(0.7, 0.95),
                'recommended_fine_tuning_epochs': np.random.randint(5, 20)
            }
        }
        
        # Store results
        optimization_jobs[adaptation_id]['status'] = 'completed'
        optimization_jobs[adaptation_id]['results'] = adaptation_results
        optimization_jobs[adaptation_id]['progress'] = 100.0
        optimization_jobs[adaptation_id]['completed_at'] = datetime.now()
        
        logger.info(f"Meta-learning adaptation {adaptation_id} completed successfully")
        
    except Exception as e:
        logger.error(f"Meta-learning adaptation {adaptation_id} failed: {e}")
        optimization_jobs[adaptation_id]['status'] = 'failed'
        optimization_jobs[adaptation_id]['error'] = str(e)
    
    finally:
        # Cleanup meta-learner
        if adaptation_id in meta_learners:
            del meta_learners[adaptation_id]


async def _run_adaptive_strategy_management(
    manager_id: str,
    adaptive_manager: AdaptiveStrategyManager,
    request: AdaptiveStrategyRequest
):
    """Run adaptive strategy management in background."""
    try:
        # Mock adaptive strategy management
        adaptations_made = 0
        
        # Simulate continuous adaptation process
        for i in range(10):  # Simulate 10 adaptation cycles
            await asyncio.sleep(0.5)  # Simulate monitoring time
            
            # Mock market regime detection
            current_regime = np.random.choice(['trending', 'sideways', 'volatile'])
            
            # Mock strategy performance
            current_performance = np.random.uniform(0.0, 1.0)
            
            # Decide if adaptation is needed
            if current_performance < request.performance_threshold:
                # Select new strategy
                new_strategy = np.random.choice(request.strategy_pool)
                adaptations_made += 1
                
                # Update job info
                optimization_jobs[manager_id]['current_strategy'] = new_strategy
                optimization_jobs[manager_id]['adaptations_count'] = adaptations_made
                
                logger.info(f"Adaptive manager {manager_id} switched to strategy: {new_strategy}")
            
            # Update progress
            optimization_jobs[manager_id]['progress'] = (i + 1) * 10.0
        
        # Final results
        adaptation_results = {
            'total_adaptations': adaptations_made,
            'final_strategy': optimization_jobs[manager_id]['current_strategy'],
            'average_performance': np.random.uniform(0.6, 0.9),
            'regime_changes_detected': np.random.randint(3, 8),
            'adaptation_success_rate': np.random.uniform(0.7, 0.95),
            'performance_improvement': np.random.uniform(0.1, 0.4)
        }
        
        # Store results
        optimization_jobs[manager_id]['status'] = 'completed'
        optimization_jobs[manager_id]['results'] = adaptation_results
        optimization_jobs[manager_id]['progress'] = 100.0
        optimization_jobs[manager_id]['completed_at'] = datetime.now()
        
        logger.info(f"Adaptive strategy management {manager_id} completed successfully")
        
    except Exception as e:
        logger.error(f"Adaptive strategy management {manager_id} failed: {e}")
        optimization_jobs[manager_id]['status'] = 'failed'
        optimization_jobs[manager_id]['error'] = str(e)
    
    finally:
        # Cleanup adaptive manager
        if manager_id in adaptive_managers:
            del adaptive_managers[manager_id]