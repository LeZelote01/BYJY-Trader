#!/usr/bin/env python3
"""
üß† PHASE 2.2 - ENTRA√éNEMENT LSTM AVEC YAHOO FINANCE
Entra√Ænement LSTM avec donn√©es directes de Yahoo Finance
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import joblib
import yfinance as yf
from datetime import datetime
from pathlib import Path

print("üöÄ PHASE 2.2 - ENTRA√éNEMENT LSTM AVEC YAHOO FINANCE")
print("=" * 60)

# Configuration selon roadmap
SYMBOL = "AAPL"
SEQUENCE_LENGTH = 60
EPOCHS = 25  # Test rapide
BATCH_SIZE = 32
TEST_SIZE = 0.2

# Param√®tres mod√®le selon PHASE2_2_SPEC_AI_MODELS.md
LSTM_UNITS = [128, 64, 32]
DROPOUT = 0.3
LEARNING_RATE = 0.001

def load_yahoo_data():
    """Charge les donn√©es depuis Yahoo Finance"""
    print(f"üìä Chargement donn√©es {SYMBOL} depuis Yahoo Finance...")
    
    try:
        # T√©l√©chargement 1 an de donn√©es (6+ mois comme demand√©)
        ticker = yf.Ticker(SYMBOL)
        df = ticker.history(period="1y")
        
        if len(df) == 0:
            print(f"‚ùå Aucune donn√©e trouv√©e pour {SYMBOL}")
            return None
            
        print(f"‚úÖ {len(df)} points de donn√©es charg√©s")
        print(f"üìÖ P√©riode: {df.index.min().date()} ‚Üí {df.index.max().date()}")
        print(f"üìä Prix: ${df['Close'].min():.2f} ‚Üí ${df['Close'].max():.2f}")
        
        return df
        
    except Exception as e:
        print(f"‚ùå Erreur chargement: {e}")
        return None

def create_sequences(data, sequence_length):
    """Cr√©e les s√©quences temporelles pour LSTM"""
    X, y = [], []
    for i in range(sequence_length, len(data)):
        X.append(data[i-sequence_length:i])
        y.append(data[i])
    return np.array(X), np.array(y)

def train_lstm_model():
    """Entra√Æne le mod√®le LSTM basique"""
    
    # 1. Chargement des donn√©es
    df = load_yahoo_data()
    if df is None:
        return False
        
    # 2. Pr√©paration des donn√©es
    print("\nüîß Pr√©paration donn√©es LSTM...")
    
    # Utiliser les prix de cl√¥ture
    prices = df['Close'].values
    print(f"üìà {len(prices)} prix de cl√¥ture r√©cup√©r√©s")
    
    # Normalisation MinMaxScaler [0,1]
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_prices = scaler.fit_transform(prices.reshape(-1, 1)).flatten()
    print(f"‚úÖ Normalisation: [{scaled_prices.min():.3f}, {scaled_prices.max():.3f}]")
    
    # Cr√©ation s√©quences temporelles
    X, y = create_sequences(scaled_prices, SEQUENCE_LENGTH)
    print(f"üìä S√©quences cr√©√©es: {X.shape[0]} s√©quences de {SEQUENCE_LENGTH} jours")
    
    # Division train/test
    split_idx = int(len(X) * (1 - TEST_SIZE))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # Reshape pour LSTM [samples, time steps, features]
    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    print(f"‚úÖ Division: Train {X_train.shape}, Test {X_test.shape}")
    
    # 3. Cr√©ation du mod√®le LSTM
    print("\nüß† Construction mod√®le LSTM...")
    
    model = Sequential([
        # Premi√®re couche LSTM avec return_sequences=True
        LSTM(LSTM_UNITS[0], return_sequences=True, input_shape=(SEQUENCE_LENGTH, 1)),
        Dropout(DROPOUT),
        
        # Deuxi√®me couche LSTM
        LSTM(LSTM_UNITS[1], return_sequences=True),
        Dropout(DROPOUT),
        
        # Troisi√®me couche LSTM  
        LSTM(LSTM_UNITS[2], return_sequences=False),
        Dropout(DROPOUT),
        
        # Couche Dense de sortie
        Dense(1)
    ])
    
    # Compilation avec optimiseur Adam
    model.compile(
        optimizer=Adam(learning_rate=LEARNING_RATE),
        loss='mse',
        metrics=['mae']
    )
    
    print(f"‚úÖ Architecture: {LSTM_UNITS} ‚Üí 1")
    print(f"‚úÖ Param√®tres: {model.count_params():,}")
    
    # 4. Entra√Ænement du mod√®le
    print("\nüöÄ D√©marrage entra√Ænement...")
    print(f"‚öôÔ∏è  Epochs: {EPOCHS}, Batch size: {BATCH_SIZE}")
    
    start_time = datetime.now()
    
    # Callbacks pour optimiser l'entra√Ænement
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=7,
            restore_best_weights=True,
            verbose=1
        )
    ]
    
    history = model.fit(
        X_train, y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_test, y_test),
        callbacks=callbacks,
        verbose=1
    )
    
    training_time = datetime.now() - start_time
    print(f"\n‚è±Ô∏è  Entra√Ænement termin√© en {training_time}")
    
    # 5. √âvaluation du mod√®le
    print("\nüìä √âvaluation performance...")
    
    # Pr√©dictions sur test set
    predictions = model.predict(X_test, verbose=0)
    
    # D√©normalisation pour m√©triques r√©elles
    y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
    predictions_actual = scaler.inverse_transform(predictions).flatten()
    
    # Calcul m√©triques
    mse = mean_squared_error(y_test_actual, predictions_actual)
    mae = mean_absolute_error(y_test_actual, predictions_actual)
    rmse = np.sqrt(mse)
    
    # Calcul accuracy directionnelle (objectif Phase 2.2: > 60%)
    y_diff = np.diff(y_test_actual)
    pred_diff = np.diff(predictions_actual)
    directional_accuracy = np.mean(np.sign(y_diff) == np.sign(pred_diff)) * 100
    
    # Calcul RMSE relatif (objectif < 5%)
    price_std = y_test_actual.std()
    rmse_relative = (rmse / price_std) * 100
    
    print("\nüìà R√âSULTATS √âVALUATION:")
    print(f"   üí∞ RMSE: ${rmse:.2f} ({rmse_relative:.1f}% de la volatilit√©)")
    print(f"   üìä MAE: ${mae:.2f}")
    print(f"   üéØ Accuracy Directionnelle: {directional_accuracy:.1f}%")
    print(f"   üìâ Loss final: {history.history['loss'][-1]:.6f}")
    
    # Validation crit√®res Phase 2.2 selon roadmap
    accuracy_target = 60.0  # > 60% selon PHASE2_2_SPEC
    rmse_target = 5.0       # < 5% selon PHASE2_2_SPEC
    
    accuracy_met = directional_accuracy > accuracy_target
    rmse_met = rmse_relative < rmse_target
    overall_success = accuracy_met and rmse_met
    
    print(f"\nüéØ VALIDATION OBJECTIFS PHASE 2.2:")
    print(f"   üìà Accuracy > {accuracy_target}%: {'‚úÖ' if accuracy_met else '‚ùå'} ({directional_accuracy:.1f}%)")
    print(f"   üìä RMSE < {rmse_target}%: {'‚úÖ' if rmse_met else '‚ùå'} ({rmse_relative:.1f}%)")
    print(f"   üéâ Objectifs globaux: {'‚úÖ ATTEINTS' if overall_success else '‚ùå PARTIELS'}")
    
    # 6. Sauvegarde du mod√®le
    print("\nüíæ Sauvegarde du mod√®le...")
    
    # Cr√©ation dossier mod√®les
    models_dir = Path('/app/ai/trained_models')
    models_dir.mkdir(parents=True, exist_ok=True)
    
    # Sauvegarde mod√®le TensorFlow
    model_path = models_dir / f'lstm_{SYMBOL.lower()}_phase2.h5'
    model.save(model_path)
    
    # Sauvegarde scaler
    scaler_path = models_dir / f'scaler_{SYMBOL.lower()}_phase2.pkl'
    joblib.dump(scaler, scaler_path)
    
    # Sauvegarde m√©triques et m√©tadonn√©es
    metadata = {
        'symbol': SYMBOL,
        'sequence_length': SEQUENCE_LENGTH,
        'training_date': datetime.now().isoformat(),
        'epochs_trained': len(history.history['loss']),
        'model_path': str(model_path),
        'scaler_path': str(scaler_path),
        'metrics': {
            'rmse': float(rmse),
            'mae': float(mae),
            'rmse_relative_percent': float(rmse_relative),
            'directional_accuracy_percent': float(directional_accuracy),
            'final_loss': float(history.history['loss'][-1]),
            'final_val_loss': float(history.history['val_loss'][-1])
        },
        'objectives_met': {
            'accuracy_target': accuracy_target,
            'accuracy_achieved': float(directional_accuracy),
            'accuracy_met': accuracy_met,
            'rmse_target': rmse_target,
            'rmse_achieved': float(rmse_relative),
            'rmse_met': rmse_met,
            'overall_success': overall_success
        }
    }
    
    # Sauvegarde JSON
    import json
    metadata_path = models_dir / f'metadata_{SYMBOL.lower()}_phase2.json'
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"‚úÖ Mod√®le: {model_path}")
    print(f"‚úÖ Scaler: {scaler_path}")
    print(f"‚úÖ M√©tadonn√©es: {metadata_path}")
    
    # 7. R√©sum√© final
    print("\n" + "="*60)
    print("üéâ PHASE 2.2 - ENTRA√éNEMENT LSTM TERMIN√â")
    print("="*60)
    
    if overall_success:
        print("üèÜ SUCC√àS COMPLET: Tous les objectifs Phase 2.2 atteints!")
        print("üöÄ Pr√™t pour Phase 2.3: Tests pr√©dictions temps r√©el")
        print("üìà Le mod√®le peut √™tre utilis√© pour trading simulation")
    elif accuracy_met:
        print("‚úÖ SUCC√àS PARTIEL: Accuracy atteinte, RMSE √† am√©liorer")
        print("üí° Recommandation: Plus de donn√©es ou fine-tuning hyperparam√®tres")  
    else:
        print("‚ö†Ô∏è  OBJECTIFS PARTIELLEMENT ATTEINTS")
        print("üí° Le mod√®le de base est fonctionnel pour tests")
        print("üîß Recommandations: √âtendre dataset ou ajuster architecture")
    
    print(f"\nüìä Mod√®le final: {directional_accuracy:.1f}% accuracy, {rmse_relative:.1f}% RMSE")
    print("üéØ Prochaine √©tape selon Roadmap: Configuration mod√®les ensemble")
    
    return overall_success

if __name__ == "__main__":
    success = train_lstm_model()
    print(f"\nüîö Statut final: {'‚úÖ SUCCESS' if success else '‚ö†Ô∏è PARTIAL'}")