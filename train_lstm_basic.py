#!/usr/bin/env python3
"""
üß† PHASE 2.2 - ENTRA√éNEMENT MOD√àLE LSTM BASIQUE
Entra√Ænement d'un mod√®le LSTM sur donn√©es AAPL selon roadmap
"""

import sys
import os
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path

# Configuration du chemin
sys.path.insert(0, '/app')

# Importation des modules BYJY-Trader  
from core.config import get_settings
from core.database import get_database_manager
from data.storage.data_manager import DataManager

# Imports IA/ML
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import joblib

print("üöÄ PHASE 2.2 - ENTRA√éNEMENT LSTM BASIQUE")
print("=" * 50)

# Configuration
settings = get_settings()
db_manager = get_database_manager()
data_manager = DataManager()

# Param√®tres LSTM selon sp√©cifications
SYMBOL = "AAPL"
SEQUENCE_LENGTH = 60  # 60 jours selon roadmap
EPOCHS = 50
BATCH_SIZE = 32
TEST_SIZE = 0.2
RANDOM_STATE = 42

# Param√®tres mod√®le selon PHASE2_2_SPEC_AI_MODELS.md
LSTM_UNITS = [128, 64, 32]  # 3 couches LSTM
DROPOUT = 0.3
LEARNING_RATE = 0.001

async def load_training_data():
    """Charge les donn√©es d'entra√Ænement depuis la base"""
    print(f"üìä Chargement donn√©es {SYMBOL}...")
    
    try:
        # R√©cup√©ration des donn√©es via DataManager
        end_date = datetime.now()
        start_date = end_date - timedelta(days=365)  # 12 mois pour l'entra√Ænement
        
        data = await data_manager.get_historical_data(
            symbol=SYMBOL,
            start_date=start_date,
            end_date=end_date,
            interval='1d'
        )
        
        if data is None or len(data) == 0:
            print(f"‚ùå Aucune donn√©e trouv√©e pour {SYMBOL}")
            return None
            
        # Conversion en DataFrame
        df = pd.DataFrame(data)
        print(f"‚úÖ {len(df)} points de donn√©es charg√©s")
        print(f"üìÖ P√©riode: {df['timestamp'].min()} ‚Üí {df['timestamp'].max()}")
        
        return df
        
    except Exception as e:
        print(f"‚ùå Erreur chargement donn√©es: {e}")
        return None

def prepare_lstm_data(df, target_column='close'):
    """Pr√©pare les donn√©es pour l'entra√Ænement LSTM"""
    print(f"üîß Pr√©paration donn√©es LSTM...")
    
    # Utiliser uniquement les prix de cl√¥ture
    data = df[target_column].values.reshape(-1, 1)
    
    # Normalisation avec MinMaxScaler
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(data)
    
    # Cr√©ation des s√©quences
    X, y = [], []
    for i in range(SEQUENCE_LENGTH, len(scaled_data)):
        X.append(scaled_data[i-SEQUENCE_LENGTH:i, 0])
        y.append(scaled_data[i, 0])
    
    X, y = np.array(X), np.array(y)
    
    # Division train/test
    split_idx = int(len(X) * (1 - TEST_SIZE))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # Reshape pour LSTM [samples, time steps, features]
    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    print(f"‚úÖ Donn√©es pr√©par√©es:")
    print(f"   - Train: {X_train.shape}, Test: {X_test.shape}")
    print(f"   - S√©quences: {SEQUENCE_LENGTH} jours")
    print(f"   - Normalisation: MinMaxScaler [0,1]")
    
    return X_train, X_test, y_train, y_test, scaler

def create_lstm_model():
    """Cr√©e le mod√®le LSTM selon sp√©cifications"""
    print("üß† Cr√©ation mod√®le LSTM...")
    
    model = Sequential([
        # Premi√®re couche LSTM
        LSTM(LSTM_UNITS[0], return_sequences=True, input_shape=(SEQUENCE_LENGTH, 1)),
        Dropout(DROPOUT),
        
        # Deuxi√®me couche LSTM  
        LSTM(LSTM_UNITS[1], return_sequences=True),
        Dropout(DROPOUT),
        
        # Troisi√®me couche LSTM
        LSTM(LSTM_UNITS[2], return_sequences=False),
        Dropout(DROPOUT),
        
        # Couche Dense de sortie
        Dense(1)
    ])
    
    # Compilation avec optimiseur Adam
    model.compile(
        optimizer=Adam(learning_rate=LEARNING_RATE),
        loss='mse',
        metrics=['mae']
    )
    
    print("‚úÖ Mod√®le LSTM cr√©√©:")
    print(f"   - Architecture: {LSTM_UNITS[0]}‚Üí{LSTM_UNITS[1]}‚Üí{LSTM_UNITS[2]}‚Üí1")
    print(f"   - Dropout: {DROPOUT}")
    print(f"   - Optimiseur: Adam (lr={LEARNING_RATE})")
    print(f"   - Loss: MSE")
    
    return model

def train_lstm_model(model, X_train, y_train, X_test, y_test):
    """Entra√Æne le mod√®le LSTM"""
    print("üöÄ Entra√Ænement du mod√®le LSTM...")
    
    # Callbacks pour l'entra√Ænement
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-7
        )
    ]
    
    # Entra√Ænement
    start_time = datetime.now()
    history = model.fit(
        X_train, y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_test, y_test),
        callbacks=callbacks,
        verbose=1
    )
    
    training_time = datetime.now() - start_time
    print(f"‚úÖ Entra√Ænement termin√© en {training_time}")
    
    return model, history

def evaluate_model(model, X_test, y_test, scaler):
    """√âvalue la performance du mod√®le"""
    print("üìä √âvaluation du mod√®le...")
    
    # Pr√©dictions
    predictions = model.predict(X_test)
    
    # D√©normalisation
    y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))
    predictions_actual = scaler.inverse_transform(predictions)
    
    # M√©triques
    mse = mean_squared_error(y_test_actual, predictions_actual)
    mae = mean_absolute_error(y_test_actual, predictions_actual)
    rmse = np.sqrt(mse)
    
    # Calcul accuracy directionnelle (Phase 2.2 target: >60%)
    y_diff_actual = np.diff(y_test_actual.flatten())
    pred_diff = np.diff(predictions_actual.flatten())
    directional_accuracy = np.mean(np.sign(y_diff_actual) == np.sign(pred_diff)) * 100
    
    print("üìà R√âSULTATS √âVALUATION:")
    print(f"   - RMSE: ${rmse:.2f}")
    print(f"   - MAE: ${mae:.2f}")
    print(f"   - MSE: {mse:.2f}")
    print(f"   - Accuracy Directionnelle: {directional_accuracy:.1f}%")
    
    # Validation crit√®res Phase 2.2
    target_accuracy = 60.0  # Selon roadmap
    rmse_target = y_test_actual.std() * 0.05  # <5% √©cart selon spec
    
    print("\nüéØ VALIDATION CRIT√àRES PHASE 2.2:")
    print(f"   - Accuracy > 60%: {'‚úÖ' if directional_accuracy > target_accuracy else '‚ùå'} ({directional_accuracy:.1f}%)")
    print(f"   - RMSE < 5% std: {'‚úÖ' if rmse < rmse_target else '‚ùå'} ({rmse:.2f} vs {rmse_target:.2f})")
    
    return {
        'rmse': rmse,
        'mae': mae, 
        'mse': mse,
        'directional_accuracy': directional_accuracy,
        'target_met': directional_accuracy > target_accuracy and rmse < rmse_target
    }

def save_model(model, scaler, metrics):
    """Sauvegarde le mod√®le et les m√©triques"""
    print("üíæ Sauvegarde du mod√®le...")
    
    # Cr√©er le dossier models s'il n'existe pas
    models_dir = Path('/app/ai/trained_models')
    models_dir.mkdir(parents=True, exist_ok=True)
    
    # Sauvegarde mod√®le
    model_path = models_dir / f'lstm_{SYMBOL.lower()}_basic.h5'
    model.save(model_path)
    
    # Sauvegarde scaler
    scaler_path = models_dir / f'scaler_{SYMBOL.lower()}_basic.pkl'
    joblib.dump(scaler, scaler_path)
    
    # Sauvegarde m√©triques
    metrics_path = models_dir / f'metrics_{SYMBOL.lower()}_basic.json'
    import json
    with open(metrics_path, 'w') as f:
        json.dump({
            **metrics,
            'model_path': str(model_path),
            'scaler_path': str(scaler_path),
            'training_date': datetime.now().isoformat(),
            'symbol': SYMBOL,
            'sequence_length': SEQUENCE_LENGTH
        }, f, indent=2)
    
    print(f"‚úÖ Mod√®le sauvegard√©:")
    print(f"   - Mod√®le: {model_path}")
    print(f"   - Scaler: {scaler_path}")
    print(f"   - M√©triques: {metrics_path}")

async def main():
    """Fonction principale d'entra√Ænement LSTM"""
    try:
        print(f"üéØ OBJECTIF PHASE 2.2: Accuracy > 60%, RMSE < 5%")
        print(f"üìä Symbol: {SYMBOL}, S√©quence: {SEQUENCE_LENGTH}, Epochs: {EPOCHS}")
        
        # 1. Chargement des donn√©es
        df = await load_training_data()
        if df is None:
            return False
            
        # 2. Pr√©paration des donn√©es
        X_train, X_test, y_train, y_test, scaler = prepare_lstm_data(df)
        
        # 3. Cr√©ation du mod√®le
        model = create_lstm_model()
        
        # 4. Entra√Ænement
        model, history = train_lstm_model(model, X_train, y_train, X_test, y_test)
        
        # 5. √âvaluation
        metrics = evaluate_model(model, X_test, y_test, scaler)
        
        # 6. Sauvegarde
        save_model(model, scaler, metrics)
        
        # 7. R√©sum√© final
        print("\nüéâ PHASE 2.2 - ENTRA√éNEMENT LSTM TERMIN√â")
        print("=" * 50)
        if metrics['target_met']:
            print("‚úÖ SUCC√àS: Objectifs Phase 2.2 atteints!")
            print("üöÄ Pr√™t pour Phase 2.3: Mod√®les Ensemble")
        else:
            print("‚ö†Ô∏è  Objectifs partiellement atteints")
            print("üí° Recommandation: Ajuster hyperparam√®tres ou √©tendre dataset")
            
        return metrics['target_met']
        
    except Exception as e:
        print(f"‚ùå Erreur entra√Ænement: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())