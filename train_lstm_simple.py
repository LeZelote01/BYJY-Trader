#!/usr/bin/env python3
"""
ğŸ§  PHASE 2.2 - ENTRAÃNEMENT LSTM SIMPLE
EntraÃ®nement direct avec accÃ¨s SQLite
"""

import sqlite3
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import joblib
from datetime import datetime
from pathlib import Path

print("ğŸš€ PHASE 2.2 - ENTRAÃNEMENT LSTM SIMPLE")
print("=" * 50)

# Configuration
SYMBOL = "AAPL"
SEQUENCE_LENGTH = 60
EPOCHS = 30  # RÃ©duit pour test rapide
BATCH_SIZE = 32
TEST_SIZE = 0.2

# ParamÃ¨tres modÃ¨le
LSTM_UNITS = [128, 64, 32]
DROPOUT = 0.3
LEARNING_RATE = 0.001

def load_data_from_db():
    """Charge les donnÃ©es directement depuis SQLite"""
    print(f"ğŸ“Š Chargement donnÃ©es {SYMBOL} depuis SQLite...")
    
    try:
        conn = sqlite3.connect('/app/database/byjy_trader.db')
        
        # RequÃªte pour rÃ©cupÃ©rer les donnÃ©es
        query = """
        SELECT timestamp, open, high, low, close, volume 
        FROM historical_data 
        WHERE symbol = ? 
        ORDER BY timestamp ASC
        """
        
        df = pd.read_sql_query(query, conn, params=[SYMBOL])
        conn.close()
        
        if len(df) == 0:
            print(f"âŒ Aucune donnÃ©e trouvÃ©e pour {SYMBOL}")
            return None
            
        # Conversion timestamp
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.set_index('timestamp')
        
        print(f"âœ… {len(df)} points de donnÃ©es chargÃ©s")
        print(f"ğŸ“… PÃ©riode: {df.index.min()} â†’ {df.index.max()}")
        print(f"ğŸ“Š Prix min: ${df['close'].min():.2f}, max: ${df['close'].max():.2f}")
        
        return df
        
    except Exception as e:
        print(f"âŒ Erreur chargement: {e}")
        return None

def create_sequences(data, sequence_length):
    """CrÃ©e les sÃ©quences pour LSTM"""
    X, y = [], []
    for i in range(sequence_length, len(data)):
        X.append(data[i-sequence_length:i])
        y.append(data[i])
    return np.array(X), np.array(y)

def train_lstm_model():
    """EntraÃ®ne le modÃ¨le LSTM"""
    
    # 1. Chargement des donnÃ©es
    df = load_data_from_db()
    if df is None:
        return False
        
    # 2. PrÃ©paration des donnÃ©es
    print("ğŸ”§ PrÃ©paration donnÃ©es...")
    
    # Utiliser les prix de clÃ´ture
    prices = df['close'].values
    
    # Normalisation
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_prices = scaler.fit_transform(prices.reshape(-1, 1)).flatten()
    
    # CrÃ©ation sÃ©quences
    X, y = create_sequences(scaled_prices, SEQUENCE_LENGTH)
    
    # Division train/test
    split_idx = int(len(X) * (1 - TEST_SIZE))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # Reshape pour LSTM
    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    print(f"âœ… DonnÃ©es prÃ©parÃ©es: Train {X_train.shape}, Test {X_test.shape}")
    
    # 3. CrÃ©ation du modÃ¨le
    print("ğŸ§  CrÃ©ation modÃ¨le LSTM...")
    
    model = Sequential([
        LSTM(LSTM_UNITS[0], return_sequences=True, input_shape=(SEQUENCE_LENGTH, 1)),
        Dropout(DROPOUT),
        LSTM(LSTM_UNITS[1], return_sequences=True),
        Dropout(DROPOUT),
        LSTM(LSTM_UNITS[2], return_sequences=False),
        Dropout(DROPOUT),
        Dense(1)
    ])
    
    model.compile(
        optimizer=Adam(learning_rate=LEARNING_RATE),
        loss='mse',
        metrics=['mae']
    )
    
    print(f"âœ… ModÃ¨le crÃ©Ã©: {LSTM_UNITS}")
    
    # 4. EntraÃ®nement
    print("ğŸš€ EntraÃ®nement...")
    
    history = model.fit(
        X_train, y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_test, y_test),
        verbose=1
    )
    
    # 5. Ã‰valuation
    print("ğŸ“Š Ã‰valuation...")
    
    predictions = model.predict(X_test)
    
    # DÃ©normalisation
    y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))
    predictions_actual = scaler.inverse_transform(predictions)
    
    # MÃ©triques
    mse = mean_squared_error(y_test_actual, predictions_actual)
    mae = mean_absolute_error(y_test_actual, predictions_actual)
    rmse = np.sqrt(mse)
    
    # Accuracy directionnelle
    y_diff = np.diff(y_test_actual.flatten())
    pred_diff = np.diff(predictions_actual.flatten())
    directional_accuracy = np.mean(np.sign(y_diff) == np.sign(pred_diff)) * 100
    
    print("\nğŸ“ˆ RÃ‰SULTATS:")
    print(f"   - RMSE: ${rmse:.2f}")
    print(f"   - MAE: ${mae:.2f}")
    print(f"   - Accuracy Directionnelle: {directional_accuracy:.1f}%")
    
    # Validation critÃ¨res
    target_accuracy = 60.0
    success = directional_accuracy > target_accuracy
    
    print("\nğŸ¯ VALIDATION PHASE 2.2:")
    print(f"   - Accuracy > 60%: {'âœ…' if success else 'âŒ'} ({directional_accuracy:.1f}%)")
    
    # 6. Sauvegarde
    if success:
        print("ğŸ’¾ Sauvegarde du modÃ¨le...")
        
        models_dir = Path('/app/ai/trained_models')
        models_dir.mkdir(parents=True, exist_ok=True)
        
        # Sauvegarde modÃ¨le
        model_path = models_dir / f'lstm_{SYMBOL.lower()}_basic.h5'
        model.save(model_path)
        
        # Sauvegarde scaler
        scaler_path = models_dir / f'scaler_{SYMBOL.lower()}_basic.pkl'
        joblib.dump(scaler, scaler_path)
        
        print(f"âœ… ModÃ¨le sauvegardÃ©: {model_path}")
    
    print("\nğŸ‰ PHASE 2.2 - TERMINÃ‰E")
    if success:
        print("âœ… SUCCÃˆS: Objectifs atteints!")
        print("ğŸš€ PrÃªt pour Phase 2.3: Tests prÃ©dictions temps rÃ©el")
    else:
        print("âš ï¸  Objectifs partiellement atteints - ModÃ¨le de base fonctionnel")
    
    return success

if __name__ == "__main__":
    train_lstm_model()